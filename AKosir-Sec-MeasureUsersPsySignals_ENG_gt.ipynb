{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/andrejkk/TalksImgs/master/FrontSlideUpperBan.png\" style=\"float: center; width: 100%\"/>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<center>\n",
    "<h1>Measurement of Users: Psychophysiological Signals</h1>\n",
    "<br><br>\n",
    "<h3>Andrej Košir, Lucami, Fe</h3>\n",
    "<h4>Contact: andrej.kosir@fe.uni-lj.si</h4>\n",
    "<h4>Zoom https://uni-lj-si.zoom.us/J/8638167479</h4>\n",
    "</center>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">1</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:left;\">Goals</div>\n",
    "</div>\n",
    "\n",
    "## Goals\n",
    "\n",
    "- to learn basic psychophysiological signals and their usefulness in monitoring users\n",
    "- to learn about selected sensors of psychophysiological scenes\n",
    "- To learn the basic techniques of modeling psychophysiological signals with machine learning\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">2</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Goals, offer</div>\n",
    "</div>\n",
    "\n",
    "## Content\n",
    "\n",
    "### 1. Physiology and user signals\n",
    "\n",
    "■ Human Physiology and Psychophysiological Signals $\\Large{*}$\n",
    "\n",
    "■ User's social signals $\\Large{*}$\n",
    "\n",
    "■ Estimation of users' social signals $\\Large{*}$\n",
    "\n",
    "■ Emotions: Signal measurement\n",
    "\n",
    "■ Emotions: Measurement\n",
    "\n",
    "■ Emotions: Affective Measurement, Noldus Face Reader tool\n",
    "\n",
    "■ Emotions: Determining affective status, tools\n",
    "\n",
    "■ Cognitive load: Measurement of signals\n",
    "\n",
    "■ Attention: Signal measurement\n",
    "\n",
    "■ Stress: Measurement of signals\n",
    "\n",
    "\n",
    "### 2. Measurement of psycho-physical responses and sensors\n",
    "\n",
    "■ Measurement of the eye\n",
    "\n",
    "■ Measuring electrodermal activity\n",
    "\n",
    "■ Measurement of the heart\n",
    "\n",
    "■ Measuring brain activity\n",
    "\n",
    "■ Measuring breathing\n",
    "\n",
    "■ Measuring gesture and poses\n",
    "\n",
    "■ Measurement of user: Living Lab (Living Lab)\n",
    "\n",
    "■ Living Laboratory: Sensor List\n",
    "\n",
    "■ Conclusion\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">3</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Human physiology and psychophysiological signals\n",
    "\n",
    "\n",
    "#### Def.: Psychophysiology is a scientific discipline that explores the relationship between physiology and cognitive processes.\n",
    "\n",
    "#### Def.: The social signals of the user are measurable user signals that are connected or given their physiological conditions.\n",
    "\n",
    "### important insights:\n",
    "- Psychophysiological signals are directly measurable\n",
    "- measure rapid changes in man - user\n",
    "- There is a link between psychophysiological signals and cognitive processes\n",
    "\n",
    "\n",
    "\n",
    "#### important physiological signals for communication\n",
    "- Eye signals: pupil size, look, flashing, ...\n",
    "- Heart signals: HR, ...\n",
    "- Skin signals: electrodermal activity\n",
    "\n",
    "\n",
    "\n",
    "#### significant estimates from these measurements:\n",
    "- Affective and emotional states\n",
    "- cognitive load\n",
    "- stress\n",
    "- attention\n",
    "- engagement\n",
    "- multimedia exposure\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">4</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ user social signals\n",
    "\n",
    "#### Def.: : Social signals are all left over from communication between man and his interlocutor (man or machine) when we eliminate the meaning of speech.There is a color of voice, a rhythm of speech, an expression on face, gestures, etc.We also use the term \"non -verbal communication\".\n",
    "\n",
    "#### Def.: : The term interviewee is used for all communication entities - man, machine, service.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/SoialSignals-Concept_SLO.png\" style=\"float: center; width: 65%\"/>\n",
    "<br>\n",
    "\n",
    "\n",
    "##### Examples of social signals are:\n",
    "- Satisfaction\n",
    "- curiosity\n",
    "- hesitation\n",
    "- ...<br>\n",
    "\n",
    "\n",
    "**Social signals** are changing with **time**, with quite different speeds.\n",
    "\n",
    "\n",
    "##### Utility\n",
    "\n",
    "Social signals are useful for\n",
    "- **real -time communication**: significant enrichment of content\n",
    "- **Adjusting the user**: Understanding the state of the user in real time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">5</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Estimation of users' social signals\n",
    "\n",
    "\n",
    "### Approaches with machine and statistical learning\n",
    "\n",
    "Objective of Assessment - Two Options:\n",
    "1. Assessment of quantitative social signal value at a given interval, e.g. to $[1, 5]$;\n",
    "2. Classification of social signal in selected classes eg _{low, medium, high}_ \n",
    "\n",
    "\n",
    "### Two steps\n",
    "\n",
    "1. Determining the characters\n",
    "2. Rating / Classification - Machine and Statistical Learning\n",
    "\n",
    "2.1.Social Signing Assessment - Models:\n",
    "- Linear regression\n",
    "- nonlinear models\n",
    "- Models forecasting time species\n",
    "\n",
    "\n",
    "2.2.Classification of Socailed Signals - Machine Learning:\n",
    "- Deep learning (does not require determination of character)\n",
    "- a method of support vectors\n",
    "- decision trees\n",
    "\n",
    "\n",
    "\n",
    "The evaluation concept is available in the picture below.\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/SocSigsFromPsySigs.png\" width=\"60%\"/>\n",
    "<figcaption>Source: SHU: A Review of Emotion Recognition Using Physiological Signals, Sensors, 2018.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">6</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Emotions: Measurement of signals\n",
    "\n",
    "\n",
    "#### Signals to measure emotions\n",
    "\n",
    "\n",
    "For communications measurable signals with sufficiently low intrusiveness:\n",
    "1. Eye tracking:\n",
    "- the size of the pupil\n",
    "- Look patterns\n",
    "2. Electrodermal skin activity\n",
    "- Affective state of arousal\n",
    "3. Heartbeat\n",
    "- heart rate speed and patterns\n",
    "\n",
    "<br><br>\n",
    "\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/HumanPsychoFiziologyEmotions.png\" width=\"80%\"/>\n",
    "<figcaption>Source: SHU: A Review of Emotion Recognition Using Physiological Signals, Sensors, 2018.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">7</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Emotions: Affective State Measurement, Facement Facial Coding System (Facial Action Coding System)\n",
    "\n",
    "Anatom Carl-Herman Hjortsjö has developed a facial activity encoding system.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/HeadNeckMuscle.jpg\" style=\"float: center; width: 40%;\"/>\n",
    "\n",
    "Anthropologist<b>Paul Ekman and Wallace V. Friesen</b> associated these actives with emotional states.It is a precise map of activity into emotional states\n",
    "\n",
    "\n",
    "<table style=\"width:30%\">\n",
    "<tr>\n",
    "<td style=\"text-align:left\"><b>Activity</b></td>\n",
    "<td style=\"text-align:left\"><b>Emotional state</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">4+5+7+23</td>\n",
    "<td style=\"text-align:left\">Anger</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">R12A+R14A</td>\n",
    "<td style=\"text-align:left\">Satisfaction</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">9+15+16</td>\n",
    "<td style=\"text-align:left\">Disgust</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">1+2+4+5+7+20+26</td>\n",
    "<td style=\"text-align:left\">Fear</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">6+12</td>\n",
    "<td style=\"text-align:left\">Happiness</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">1+4+15</td>\n",
    "<td style=\"text-align:left\">Sorrow</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">1+2+5b+26</td>\n",
    "<td style=\"text-align:left\">Surprise</td>\n",
    "</tr>\n",
    "</table>\n",
    "<br>There are<b>coding course</b>, where you learn to follow these codes.\n",
    "\n",
    "The FACS system is also the basis for computer programs that, based on video analysis, recognize the affective state (vad) and the selected emotional state, e.g. [NOLDUS FACE READER](http//www.noldus.com/human-behavior-research/products/facereader).\n",
    "\n",
    "<br>\n",
    "\n",
    "_Literature_: Ekman P. Facial Signs of Emotional Experience, Journal of Personality and Social Psychology, 1980.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">8</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Emotions: Affective State Measurement, Deepface Library and Noldus Face Reader tool\n",
    "\n",
    "\n",
    "Python Library [Deepface](https://pypi.org/project/deepface/).\n",
    "\n",
    "Licensed tool [NOLDUS](https://www.noldus.com/human-behavior-research).\n",
    "\n",
    "\n",
    "\n",
    "User interface appearance:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/NoldusFaceReader_GUI.jpg\" style=\"float: center; width: 40%;\"/>\n",
    "\n",
    "Basic approach:\n",
    "- fit 3D mesh on the face, giving good robustness\n",
    "- Identification of Ecman's Points (FACS)\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/NoldusFaceReader_Mesh.jpg\" style=\"float: center; width: 40%;\"/>\n",
    "\n",
    "\n",
    "##### Functionality:\n",
    "- measurement of an affective state in the exercise space, real time measurement A and D;\n",
    "- map into emotional space - circumplex model\n",
    "- time federal coordinates of 49 facial points\n",
    "- Time federal graphs\n",
    "- export of data to file\n",
    "\n",
    "\n",
    "#### Advantages:\n",
    "- a functioning system\n",
    "- many studies of evaluation of sues\n",
    "\n",
    "\n",
    "#### Weaknesses:\n",
    "- The position of the viewer in front of the camera must be in quite narrow frames that prevent the wider scenarios of testing - the test person necessarily sits at the table\n",
    "- Some studies have shown that measurement results are at least occasionally wrong\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">9</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Emotions: Measuring an affective state, success of modality in measuring affective state\n",
    "\n",
    "The table below gives a list of performance performance.The \"control\" property means how much a user can consciously control this modality (manages).<table style=\"width:95%\">\n",
    "<tr>\n",
    "<td style=\"text-align:left\"><b>Resource</b></td>\n",
    "<td style=\"text-align:left\"><b>A</b></td>\n",
    "<td style=\"text-align:left\"><b>Into</b></td>\n",
    "<td style=\"text-align:left\"><b>Self -control</b></td>\n",
    "<td style=\"text-align:left\"><b>Advantages</b></td>\n",
    "<td style=\"text-align:left\"><b>Weaknesses</b></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Audio signal</td>\n",
    "<td style=\"text-align:left\">+++</td>\n",
    "<td style=\"text-align:left\">++</td>\n",
    "<td style=\"text-align:left\">Tall</td>\n",
    "<td style=\"text-align:left\">Reliable, easy</td>\n",
    "<td style=\"text-align:left\">Few times available</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Language (colorful)</td>\n",
    "<td style=\"text-align:left\">++</td>\n",
    "<td style=\"text-align:left\">+++</td>\n",
    "<td style=\"text-align:left\">Tall</td>\n",
    "<td style=\"text-align:left\">Tied to the content of speech</td>\n",
    "<td style=\"text-align:left\">The recognition is not the best.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Visual signal</td>\n",
    "<td style=\"text-align:left\">++</td>\n",
    "<td style=\"text-align:left\">+++</td>\n",
    "<td style=\"text-align:left\">Low</td>\n",
    "<td style=\"text-align:left\">Usually available</td>\n",
    "<td style=\"text-align:left\">A weak character description</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Psychosiolle</td>\n",
    "<td style=\"text-align:left\">+</td>\n",
    "<td style=\"text-align:left\">+</td>\n",
    "<td style=\"text-align:left\">Absent</td>\n",
    "<td style=\"text-align:left\">Robust</td>\n",
    "<td style=\"text-align:left\">Intrusively</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Brain signals</td>\n",
    "<td style=\"text-align:left\">+</td>\n",
    "<td style=\"text-align:left\">++</td>\n",
    "<td style=\"text-align:left\">Absent</td>\n",
    "<td style=\"text-align:left\">High potential</td>\n",
    "<td style=\"text-align:left\">Intrusively</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Tactile sigals</td>\n",
    "<td style=\"text-align:left\">++</td>\n",
    "<td style=\"text-align:left\">+</td>\n",
    "<td style=\"text-align:left\">low</td>\n",
    "<td style=\"text-align:left\">Robust, non -intruzive</td>\n",
    "<td style=\"text-align:left\">Is often not available.</td>\n",
    "</tr>\n",
    "<caption>Table: Performance of the Affective State of Spatial Planning for individual Origin Sources</caption>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">11</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Cognitive load: Measurement of signals\n",
    "\n",
    "\n",
    "#### Signals to measure\n",
    "1. Eye tracking:\n",
    "- the size of the pupil\n",
    "- Look patterns\n",
    "2. Heartbeat\n",
    "- Variability of heart rate\n",
    "3. Electrodermal activity (EDA)\n",
    "- greater eda means a greater cognitive load\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/PsySignalsCognitiveLoad.jpg\" width=\"50%\"/>\n",
    "<figcaption>Source: Ahmad and Dr: A Framework to Estimate Cognitive Load Using Physiological Data, Personal and Ubiquitous Computing, 2020.<br>Source: Johenseen: Psychophysiologic Measures of Cognitive Load and Physical Team Leaders\n",
    "During Trauma Resuscitation, Computers and Human Behaviour, 2020.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">12</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Attention: Signal measurement\n",
    "\n",
    "\n",
    "#### Signals to measure attention:\n",
    "1. Following an eye\n",
    "- Look patterns\n",
    "- the size of the pupil\n",
    "2. Clicked:\n",
    "- samples of mouse movements\n",
    "- Typing patterns\n",
    "3. Face recognition:\n",
    "- Based on \"Facial Action Coding System (FACS), attention can be recognized\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/PsySignalsAttention.png\" width=\"40%\"/>\n",
    "<figcaption>Source: Gamboa: Attention Classification Based on Biosignals During Standard\n",
    "Cognitive Tasks for Occupational Domains, Computers, 2022.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">13</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "# ■ Stress: Measurement of signals\n",
    "\n",
    "\n",
    "#### signals to measure stress\n",
    "\n",
    "1. Heartbeat:\n",
    "- Reduced Heart Rate Variability (HRV) means increased stress\n",
    "2. Electrodermal activity\n",
    "- greater skin conductivity means larger strains\n",
    "3. Breathing\n",
    "- Quick breathing or changing breathing patterns pose a pony increased stress\n",
    "4. Face recognition:\n",
    "- Based on \"Facial Action Coding System (FACS), stress increase can be recognized\n",
    "5. Speech analysis\n",
    "- Changes in tone and voice height represent a change in stress level\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<figure>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/HumanPsychoFiziologyStress.png\" width=\"40%\"/>\n",
    "<figcaption>Source: Giannakakis: Review on Psychological Stress Detection using Biosignals, IEEE Transactions on Affective Computing, 2022.</figcaption>\n",
    "</figure>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">14</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## 2. Measuring psycho-physiological signals\n",
    "\n",
    "■ Measurement of the eye\n",
    "\n",
    "■ Measuring electrodermal activity\n",
    "\n",
    "■ Measurement of the heart\n",
    "\n",
    "■ Measuring brain activity\n",
    "\n",
    "■ Measuring breathing\n",
    "\n",
    "■ Measuring gesture and poses\n",
    "\n",
    "■ Measurement of user: Living Lab (Living Lab)\n",
    "\n",
    "■ Living Laboratory: Sensor List\n",
    "\n",
    "■ Conclusion\n",
    "\n",
    "\n",
    "_Literature: _ Lee J. D., Wickens C. D., Liu Y., and Boyle L. N., Designing for People: An Introduction to Human Factors Engineering, 3rd Edition.Charleston, SC: Createspace, 2017.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">15</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Measurement of the eye\n",
    "\n",
    "#### We measure\n",
    "- pupil size: cognitive load\n",
    "- gaze: Attention, ...\n",
    "- events: cuts, fixation, ..\n",
    "\n",
    "\n",
    "#### Selected Properties\n",
    "- secades and fixations\n",
    "\n",
    "\n",
    "\n",
    "#### sensors, tools\n",
    "- Tobbi: https://www.tobii.com/, analytical tools in cloud\n",
    "- PupilLabs: https://pupil-labs.com/, analytical tools in cloud\n",
    "\n",
    "\n",
    "#### Example signal\n",
    "\n",
    "###### Pupil size (at time $0$ we start with the \"n-back\" task)\n",
    "<img width=\"60%\" src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/PupilDiameter_change.png\">\n",
    "\n",
    "###### Cascades and fixations\n",
    "<img width=\"40%\" src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/EyeFixations.jpg\">\n",
    "\n",
    "<br><br>\n",
    "_Literature_ : Wikipedia website https://en.wikipedia.org/wiki/Saccade <br>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">16</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Measuring electrodermal activity\n",
    "\n",
    "#### We measure\n",
    "- skin impedance at different frequencies\n",
    "\n",
    "\n",
    "#### analysis\n",
    "- Phasic Skin Conductance Respons\n",
    "- Tonic component (Tonic Skin Conductance Level (SCL)): slowly changing signal and continuous\n",
    "\n",
    "We are interested in the phase component, tonically eliminated for a more accurate assessment of the phase component.\n",
    "\n",
    "\n",
    "#### tools\n",
    "- Python Neurokit: https://neurokit2.readthedocs.io/en/latest/index.html\n",
    "\n",
    "\n",
    "#### Signal examples\n",
    "\n",
    "Electrodermal activity and decomposition to the phase and tonic component.<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/EDA_Decomposition.png\" width=\"80%\"/>\n",
    "<br><br>_Literatura_: Wikipedia website https://en.wikipedia.org/wiki/electrodermal_Activity<br>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">17</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Heart measurement\n",
    "\n",
    "\n",
    "#### We measure\n",
    "- electrical signal of heart muscle (electrical cardiogram)\n",
    "\n",
    "\n",
    "#### analysis\n",
    "- P, Q, S and T Waves: Four stages of contraction and stretching of the heart muscle\n",
    "\n",
    "\n",
    "#### tools\n",
    "- Python Neurokit: https://neurokit2.readthedocs.io/en/latest/index.html\n",
    "\n",
    "\n",
    "#### Signal examples\n",
    "\n",
    "Points P and Q.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/Heart_PQpoints.png\" width=\"60%\"/>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "_Literature_: Wikipedia website https://en.wikipedia.org/wiki/electrocardiography<br>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">18</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Measuring breathing\n",
    "\n",
    "#### We measure\n",
    "- the chest movement\n",
    "\n",
    "\n",
    "#### Analysis\n",
    "- Respiratory variability\n",
    "- Frequency RRV analysis\n",
    "- Poincare's diagram\n",
    "\n",
    "#### Tools\n",
    "\n",
    "Python Neurokit: https://neurokit2.readthedocs.io/en/latest/index.html\n",
    "\n",
    "\n",
    "\n",
    "#### Signal examples\n",
    "\n",
    "Breathing signal:\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/Respiration_rate.png\" width=\"80%\"/>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "_Literatura_: Wikipedia website https://en.wikipedia.org/wiki/resirator_rate<br>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">19</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Measuring gesture and poses\n",
    "\n",
    "\n",
    "#### We measure\n",
    "- RGB and IR cameras measure part of the body or whole body and determine the skeleton.\n",
    "\n",
    "\n",
    "#### analysis\n",
    "- Tracking of movement, behavior\n",
    "\n",
    "\n",
    "#### Example Isignal\n",
    "The skeleton of the user\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/KinectSkeleton.png\" width=\"40%\"/>\n",
    "\n",
    "<br><br>\n",
    "\n",
    "_Literature_: Wikipedia website https://en.wikipedia.org/wiki/electroCECEHALHALOGRAGHY<br>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">21</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ User measurement: Living lab (Living Lab)<br><br>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/LivingLab.png\" width=\"90%\"/>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">22</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;\">Measurement of Users: Psychophysiological Signals</div>\n",
    "</div>\n",
    "\n",
    "## ■ Living lab: a list of sensors\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<table style=\"width:80%\">\n",
    "<tr>\n",
    "<th style=\"text-align:left\">Measured signal</th>\n",
    "<th style=\"text-align:left\">An estimated signal</th>\n",
    "<th style=\"text-align:left\">Sensor</th>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Video</td>\n",
    "<td style=\"text-align:left\">Affective, emotional state</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/CameraRGB.png\" width=\"40%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Electrodermal activity</td>\n",
    "<td style=\"text-align:left\">Stress</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/EmpaticaEmbracePlus.png\" width=\"40%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Electrodermal activity</td>\n",
    "<td style=\"text-align:left\">Stress</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/ShimmerSensor.png\" width=\"40%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Pupil size</td>\n",
    "<td style=\"text-align:left\">Cognitive load</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/TobiiGlasses.png\" width=\"60%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Pupil size</td>\n",
    "<td style=\"text-align:left\">Cognitive load</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/SensorTobiiEyeTrackerNano.jpg\" width=\"60%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Gesture and pose</td>\n",
    "<td style=\"text-align:left\">Managing UI, social signals</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/Kinect3.png\" width=\"40%\"/></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td style=\"text-align:left\">Gesture and pose</td>\n",
    "<td style=\"text-align:left\">Managing UI, social signals</td>\n",
    "<td style=\"text-align:left\"><img src=\"https://raw.githubusercontent.com/andrejkk/UPK_DataImgs/master/CameraLuxonisOAK.png\" width=\"40%\"/></td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "<p style=\"margin-bottom:2cm;\"></p>\n",
    "<div style=\"width:100%; text-align: right; font-weight:bold; font-size:1.2em;\">23</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/></be>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"display:flex;font-weight:bold;font-size:0.9em;\">\n",
    "<div style=\"flex:1;width:50%;text-align:right;\"> Merjenje uporabnikov: psihofiziološki signali  </div>\n",
    "</div>\n",
    "\n",
    "# ■ Conclusions\n",
    "\n",
    "\n",
    "- Psychophysiological sensors allow the selected social signals to be evaluated\n",
    "- Sensor's annoyance is essential: suitability for real apps\n",
    "\n",
    "<p style=\"margin-bottom:1cm;\"></p>\n",
    "<div style=\"width:100%;text-align:right;font-weight:bold;font-size:1.2em;\">24</div>\n",
    "<img src=\"https://raw.githubusercontent.com/andrejkk/ORvTK_SlidesImgs/master/footer_full.jpg\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3.9.7 ('spyder_tf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f16579df0db49a298d8f1c71e3c9674adcc712ea2693c640f002e39896cc0aaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
